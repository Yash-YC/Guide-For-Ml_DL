{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing/Exploding Gradients in Deep Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vanishing \n",
    "\n",
    "As the backpropagation algorithm advances downwards(or backward) from the output layer towards the input layer, the gradients often get smaller and smaller and approach zero which eventually leaves the weights of the initial or lower layers nearly unchanged. As a result, the gradient descent never converges to the optimum. This is known as the vanishing gradients problem.\n",
    "\n",
    "- Exploding \n",
    "\n",
    "On the contrary, in some cases, the gradients keep on getting larger and larger as the backpropagation algorithm progresses. This, in turn, causes very large weight updates and causes the gradient descent to diverge. This is known as the exploding gradients problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to know if our model is suffering from the Exploding/Vanishing gradient problem?\n",
    "- Exploding\n",
    "\n",
    "There is an exponential growth in the model parameters.\n",
    "\n",
    "The model weights may become NaN during training.\n",
    "\n",
    "The model experiences  avalanche learning.\t\t\n",
    "\n",
    "- Vanishing\n",
    "\n",
    "The parameters of the higher layers change significantly whereas the parameters of lower layers would not change much (or not at all).\n",
    "\n",
    "The model weights may become 0 during training.\n",
    "\n",
    "The model learns very slowly and perhaps the training stagnates at a very early stage just after a few iterations.\n",
    "\n",
    "\n",
    "Certainly, neither do we want our signal to explode or saturate nor do we want it to die out. The signal needs to flow properly both in the forward direction when making predictions as well as in the backward direction while calculating gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Solutions\n",
    "\n",
    "Proper weight initialization\n",
    "\n",
    "Using non-saturating activation functions\n",
    "\n",
    "Batch normalization\n",
    "\n",
    "Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Further reading [Link](https://www.analyticsvidhya.com/blog/2021/06/the-challenge-of-vanishing-exploding-gradients-in-deep-neural-networks/)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a460da068ad21e825cd853dee63733b8b91400b72d458b93ad7c646498b211f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
